# -*- coding: utf-8 -*-
"""LSTM_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1636MakbZH1gPnN3Rek10CqsVZc5J9YDj

# Loading the dataset from Drive
"""

!gdown 1FLgYn0VxvZjCI_mF_4oJ6rI4SGxcNKl3
!tar -xf test.tar
!rm test.tar

"""# Loading the model parameters from Drive"""

!gdown 1H6cudwNit4ZENI2EmU2tDK6MdCDkl-hU
!gdown 1f36AZptw_A97Nxql4gc7HDGoVVbbQDx0
!gdown 11LlUD5dEs7iLqTQCJ1MP6mLuTRd1_OFZ

"""# Inference Code"""

import torch,warnings
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import recall_score,precision_score
import matplotlib.pyplot as plt
import numpy as np
warnings.filterwarnings('ignore')

class KD_Dataset(Dataset):
    """
    Dataset for multilabel classification with binary vectors and time series data
    """
    def __init__(
        self,
        binary_labels: np.ndarray,  # Shape: (n_samples, n_classes)
        time_series_data: np.ndarray, # Shape: (n_samples, seq_len, num_channels)
    ):
        self.binary_labels = torch.tensor(binary_labels, dtype=torch.float32)
        self.time_series_data = torch.tensor(time_series_data, dtype=torch.float32)

    def __len__(self):
        return len(self.binary_labels)

    def __getitem__(self, idx):

        inputs={}
        inputs['labels'] = self.binary_labels[idx]
        # Add time series data
        inputs['time_series_data'] = self.time_series_data[idx]
        return inputs

def squeeze_array(GT, PRD, w):
    n = len(GT)
    ones = np.where(GT == 1)[0]
    if ones.size == 0:
        return np.array([0], dtype=int)

    intervals = [(max(0, i - w), min(n - 1, i + w)) for i in ones]
    intervals.sort(key=lambda x: x[0])
    modf_GT = []
    modf_PRD=[]
    if intervals[0][0] > 0:
       [modf_GT.append(0) for _ in range(intervals[0][0])]
       [modf_PRD.append(PRD[i]) for i in range(intervals[0][0])]
    i=0
    while i<len(intervals)-1:
      modf_GT.append(1)
      if sum(PRD[intervals[i][0]:intervals[i][1]+1])>0:
        modf_PRD.append(1)
      else:
        modf_PRD.append(0)
      s1, e1 = intervals[i]
      s2, e2 = intervals[i+1]
      i=i+1
      if s2-e1-1>0:
        [modf_GT.append(0) for _ in range(s2-e1-1)]
        [modf_PRD.append(PRD[j]) for j in range(e1+1,s2)]
    modf_GT.append(1)
    if sum(PRD[intervals[-1][0]:intervals[-1][1]+1])>0:
        modf_PRD.append(1)
    else:
        modf_PRD.append(0)
    if intervals[-1][1] <= n - 1:
      [modf_GT.append(0) for _ in range(n - intervals[-1][1] - 1)]
      [modf_PRD.append(PRD[i]) for i in range(intervals[-1][1]+1,n)]
    return np.array(modf_GT, dtype=int), np.array(modf_PRD, dtype=int)

def cut_and_infer(prob,cut_v):
  # Convert input to numpy array if it's a list
    prob = np.array(prob)

    req_intrvl=[]
    idx_above_cut=[]

    for i in range(len(prob)):
        if prob[i] > cut_v:
            idx_above_cut.append(i)

    if not idx_above_cut: # Handle case where no values are above the cut_v
        return []

    req_indx=[idx_above_cut[0]]
    for i in range(1,len(idx_above_cut)):
        if idx_above_cut[i]-idx_above_cut[i-1] > 1:
            req_indx.append(idx_above_cut[i-1])
            req_indx.append(idx_above_cut[i])
    req_indx.append(idx_above_cut[-1])
    #print(req_indx)

    req_intrvl=[(req_indx[i],req_indx[i+1]) for i in range(0,len(req_indx),2)]
    #merging intervals with a gap of 10 or less
    req_intrvl_merge=[]
    i=0
    while i <= len(req_intrvl)-2:
        if req_intrvl[i+1][0] - req_intrvl[i][1] <=10:
            req_intrvl_merge.append((req_intrvl[i][0],req_intrvl[i+1][1]))
            i=i+2
        elif req_intrvl[i][0] != req_intrvl[i][1]:  #avoiding adding (r,r) types of element
            req_intrvl_merge.append(req_intrvl[i])
            i=i+1
        else:
            i=i+1
    if len(req_intrvl)>1:
        if req_intrvl[-1][0] - req_intrvl[-2][1] >10 and req_intrvl[-1][0] != req_intrvl[-1][1]:
            req_intrvl_merge.append(req_intrvl[-1])
        elif req_intrvl[-1][0] - req_intrvl[-1][1]<=10:
            req_intrvl_merge[-1]=(req_intrvl_merge[-1][0],req_intrvl[-1][1])
    if len(req_intrvl)==1:
        req_intrvl_merge.append(req_intrvl[0])

    predicted_indices=[]
    for i in range(len(req_intrvl_merge)):
        s=req_intrvl_merge[i][0]
        e=req_intrvl_merge[i][1]
        predicted_indices.append(s+np.argmax(prob[s:e+1]))

    return np.array(predicted_indices)

class RNNAnomalyDetector(nn.Module):
    def __init__(self, input_size=2, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.3):
        super(RNNAnomalyDetector, self).__init__()

        self.rnn = nn.LSTM(
            input_size=input_size,      # number of features per timestep (2 channels)
            hidden_size=hidden_size,    # latent dimension
            num_layers=num_layers,      # stacked RNN layers
            batch_first=True,           # input: (batch, seq_len, input_size)
            bidirectional=bidirectional,
            dropout=dropout
        )


        self.num_directions = 2 if bidirectional else 1

        self.fc = nn.Linear(hidden_size * self.num_directions, 1)

    def forward(self, x):
        rnn_out, _ = self.rnn(x)

        logits = self.fc(rnn_out)

        logits = logits.squeeze(-1)
        return logits

def get_inference(test_label,test_ts_data,model_pth):

    batch_size = 1
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    # 5. Instantiate the MultilabelVLMDataset
    test_dataset = KD_Dataset(
        binary_labels=test_label,
        time_series_data=test_ts_data
    )

    # 6. Instantiate the DataLoader
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        pin_memory=True if DEVICE == 'cuda' else False
    )

    # Initialize the RNNAnomalyDetector model
    student_model = RNNAnomalyDetector(
        input_size=2,
        hidden_size=128,
        num_layers=2,
        bidirectional=True,
        dropout=0.3
    )

    state_dict=torch.load(model_pth,map_location=DEVICE)
    student_model.load_state_dict(state_dict['model_state_dict'])
    student_model.to(DEVICE)
    student_model.eval()
    try:
        for batch in test_dataloader:
            ts_batch = batch['time_series_data'].to(DEVICE)
            with torch.no_grad():
                # c. Get the student model's logits
                student_logits = student_model(ts_batch)
                # Get probabilities and predictions
                probs = torch.sigmoid(student_logits)
        probs=probs.cpu().numpy()

    except Exception as e:
        print(f"Error in test batch: {str(e)}")

    def plot_image(ind_p,ind_t,ts):

        a=np.arange(0,256)
        fig,ax=plt.subplots(2,1,figsize=(8,6))
        col=['red','blue']
        for i in range(2):
            ax[i].plot(ts[:,i],color=col[i],alpha=0.7)
            ax[i].scatter(a[ind_t],ts[ind_t,i],marker='*',color='#1E3A8A',alpha=1,label='true anomalies',s=50)
            ax[i].scatter(a[ind_p],ts[ind_p,i],marker='o',color='#7F1D1D',alpha=1,label='predicted anomalies',s=40)
            ax[i].set_xlabel('Time steps')
            ax[i].set_ylabel(f'Channel {i+1} values')
            ax[i].legend()
        plt.tight_layout()
        plt.show()

    probs=np.squeeze(probs)
    test_label=np.squeeze(test_label)
    ind_p=cut_and_infer(probs,np.mean(probs))
    ind_t=np.where(test_label==1)[0]
    print('Predicted anomalies',ind_p)
    print('True anomalies',ind_t)
    plot_image(ind_p,ind_t,np.squeeze(test_ts_data))
    pred=np.zeros(256,dtype=np.int8)
    pred[ind_p]=1
    # a,b=labels,pred
    a,b=squeeze_array(test_label,pred,2)
    score_r=recall_score(a,b,average='binary')
    score_p=precision_score(a,b,average='binary')
    print(f'Recall: {score_r:.4f} Precision: {score_p:.4f}')

"""# RNNAnomalyDetector infernce with standalone training

Loss function = BCE($\hat y$,$y_{gt}$)
"""

import pandas as pd

model_pth='/content/student_best.pth'
df=pd.read_csv('/content/test/test.csv')
num=2
ids=df.iloc[:num,0]
labels=df.iloc[:num,1:].to_numpy()
ts_arr=np.load('/content/test/test.npy')[:len(labels)]
for idx in range(num):
    print(f"\n -------------Test sample {idx+1}-------------")
    y=ts_arr[idx]
    ts=np.expand_dims(y,0)
    y=labels[idx]
    lbl=np.expand_dims(y,0)
    get_inference(lbl,ts,model_pth)

"""# RNNAnomalydetector with knowledge distillation ($\lambda=0.5$)

Loss function =$\mathcal{L}(\hat y$,$y_{gt}$)+0.5$\mathcal{L}_{KD}$
"""

import pandas as pd

model_pth='/content/kd_0.5_best.pth'
df=pd.read_csv('/content/test/test.csv')
num=2
ids=df.iloc[:num,0]
labels=df.iloc[:num,1:].to_numpy()
ts_arr=np.load('/content/test/test.npy')[:len(labels)]
for idx in range(num):
    print(f"\n -------------Test sample {idx+1}-------------")
    y=ts_arr[idx]
    ts=np.expand_dims(y,0)
    y=labels[idx]
    lbl=np.expand_dims(y,0)
    get_inference(lbl,ts,model_pth)

"""# RNNAnomalydetector with knowledge distillation only ($\lambda=1$)

Loss function =$\mathcal{L}_{KD}$
"""

import pandas as pd

model_pth='/content/kd_1_best.pth'
df=pd.read_csv('/content/test/test.csv')
num=2
ids=df.iloc[:num,0]
labels=df.iloc[:num,1:].to_numpy()
ts_arr=np.load('/content/test/test.npy')[:len(labels)]
for idx in range(num):
    print(f"\n -------------Test sample {idx+1}-------------")
    y=ts_arr[idx]
    ts=np.expand_dims(y,0)
    y=labels[idx]
    lbl=np.expand_dims(y,0)
    get_inference(lbl,ts,model_pth)

